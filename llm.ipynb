{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27900f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metrics saved as:\n",
      "- D:\\CTS Claims project\\metrics_table.csv\n",
      "- D:\\CTS Claims project\\metrics_table.json\n",
      "\n",
      "=== HUMAN-FRIENDLY EXPLANATION ===\n",
      "This claim was classified as \"not fraudulent\" because several factors suggest it's a normal claim. The claim amount is relatively low ($1000 or less) and the patient's length of stay in the hospital is short (5 days or less), which are both common characteristics of legitimate claims. Additionally, the procedure and diagnosis codes used are consistent with standard practices, further supporting the claim's legitimacy. Overall, the combination of these factors led to a low fraud score, indicating that this claim is likely genuine.\n",
      "\n",
      "=== METRICS JSON (from LLM) ===\n",
      "[\n",
      "    {\n",
      "        \"Claim ID\": \"CLM0000000001\",\n",
      "        \"Provider ID\": \"PRV001253\",\n",
      "        \"Fraud Score\": 0.1915,\n",
      "        \"Prediction\": \"NON-FRAUD\",\n",
      "        \"TP\": 55,\n",
      "        \"FP\": 65,\n",
      "        \"FN\": 75,\n",
      "        \"TN\": 85,\n",
      "        \"Avg Claim Value\": 1234.56,\n",
      "        \"Personnel Cost\": 5000,\n",
      "        \"Infra Cost\": 1500,\n",
      "        \"Compliance Cost\": 800,\n",
      "        \"Top Feature Contributions\": [\n",
      "            {\n",
      "                \"feature\": \"claim_amount <= 1000.00\",\n",
      "                \"weight\": -1.4445,\n",
      "                \"percentage\": 51.1544726963666\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"length_of_stay <= 5.00\",\n",
      "                \"weight\": 0.3402,\n",
      "                \"percentage\": 12.047595438770452\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"patient_age <= 65\",\n",
      "                \"weight\": 0.1063,\n",
      "                \"percentage\": 3.764430908704583\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"claim_duration\",\n",
      "                \"weight\": 0.0,\n",
      "                \"percentage\": 0.0\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"procedure_code_encoded\",\n",
      "                \"weight\": 0.4216,\n",
      "                \"percentage\": 14.930235852397477\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"diagnosis_code_encoded\",\n",
      "                \"weight\": 0.3477,\n",
      "                \"percentage\": 12.313194985480559\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"gender_encoded\",\n",
      "                \"weight\": 0.0057,\n",
      "                \"percentage\": 0.20185565549968132\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"comorbidity_count\",\n",
      "                \"weight\": 0.0123,\n",
      "                \"percentage\": 0.4355832566045754\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"fraud_tendency\",\n",
      "                \"weight\": 0.0561,\n",
      "                \"percentage\": 1.9866846093916\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"peer_avg_amount <= 1000.00\",\n",
      "                \"weight\": -0.0089,\n",
      "                \"percentage\": 0.31517812876266027\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"peer_std_amount <= 1000.00\",\n",
      "                \"weight\": 0.0004,\n",
      "                \"percentage\": 0.01416530915787237\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"peer_avg_los\",\n",
      "                \"weight\": -0.0801,\n",
      "                \"percentage\": 2.8366031588639427\n",
      "            },\n",
      "            {\n",
      "                \"feature\": \"peer_total_claims\",\n",
      "                \"weight\": 0.0,\n",
      "                \"percentage\": 0.0\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from groq import Groq\n",
    "\n",
    "with open(r\"D:\\CTS Claims project\\fraud_explanation.json\", \"r\") as f:\n",
    "    explainable_output = json.load(f)\n",
    "\n",
    "claim_id = explainable_output.get(\"claim_id\", \"N/A\")\n",
    "provider_id = explainable_output.get(\"provider_id\", \"N/A\")\n",
    "fraud_score = explainable_output[\"fraud_score\"]\n",
    "predicted_label = explainable_output[\"predicted_label\"]\n",
    "feature_contributions = explainable_output.get(\"feature_contributions\", {})\n",
    "metrics = explainable_output.get(\"metrics\", {})\n",
    "\n",
    "tp = metrics.get(\"tp\", 0)\n",
    "fp = metrics.get(\"fp\", 0)\n",
    "fn = metrics.get(\"fn\", 0)\n",
    "tn = metrics.get(\"tn\", 0)\n",
    "avg_claim_value = metrics.get(\"avg_claim_value\", 0)\n",
    "personnel_cost = metrics.get(\"personnel_cost\", 0)\n",
    "infra_cost = metrics.get(\"infra_cost\", 0)\n",
    "compliance_cost = metrics.get(\"compliance_cost\", 0)\n",
    "\n",
    "client = Groq(api_key=\"Your_API_KEY\") \n",
    "def explain_claim_with_llm():\n",
    "    # <<< ADDED: compute weightage for features >>>\n",
    "    if feature_contributions:\n",
    "        total_contribution = sum(abs(w) for w in feature_contributions.values()) or 1\n",
    "        weighted_contributions = {\n",
    "            feat: {\n",
    "                \"weight\": weight,\n",
    "                \"percentage\": (abs(weight) / total_contribution) * 100\n",
    "            }\n",
    "            for feat, weight in feature_contributions.items()\n",
    "        }\n",
    "        contrib_text = \"\\n\".join([\n",
    "            f\"- {feat}: {vals['weight']:.4f} ({vals['percentage']:.2f}%)\"\n",
    "            for feat, vals in weighted_contributions.items()\n",
    "        ])\n",
    "    else:\n",
    "        weighted_contributions = {}\n",
    "        contrib_text = \"No feature-level explanation available.\"\n",
    "    # <<< END ADDED >>>\n",
    "\n",
    "    explanation_prompt = f\"\"\"\n",
    "    You are an expert fraud investigator. Explain this prediction in plain English for a business user.\n",
    "\n",
    "    Claim Details:\n",
    "    - Claim ID: {claim_id}\n",
    "    - Provider ID: {provider_id}\n",
    "    - Fraud Score: {fraud_score:.4f}\n",
    "    - Prediction: {predicted_label}\n",
    "\n",
    "    Why:\n",
    "    {contrib_text}\n",
    "\n",
    "    Metrics:\n",
    "    - TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\n",
    "    - Avg Claim Value: ${avg_claim_value:,.2f}\n",
    "    - Costs: Personnel ${personnel_cost}, Infra ${infra_cost}, Compliance ${compliance_cost}\n",
    "\n",
    "    Task:\n",
    "    Explain in 3–4 sentences why this claim was classified this way, using very simple language.\n",
    "    \"\"\"\n",
    "\n",
    "    # <<< ADDED: send weightage also in table prompt >>>\n",
    "    table_prompt = f\"\"\"\n",
    "    Return ONLY a JSON array (no text, no markdown) with the following fields:\n",
    "    - Claim ID\n",
    "    - Provider ID\n",
    "    - Fraud Score\n",
    "    - Prediction\n",
    "    - TP, FP, FN, TN these values should be not in the confusion matrix values, it should be a real time value (eg: 55 )\n",
    "    - Avg Claim Value, this value should not be in minus, this should be the average of total amount claimed\n",
    "    - Personnel Cost\n",
    "    - Infra Cost\n",
    "    - Compliance Cost\n",
    "    - Top Feature Contributions (list of feature, weight, percentage)\n",
    "\n",
    "    Use this data:\n",
    "    Claim ID: {claim_id}\n",
    "    Provider ID: {provider_id}\n",
    "    Fraud Score: {fraud_score:.4f}\n",
    "    Prediction: {predicted_label}\n",
    "    TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\n",
    "    Avg Claim Value: {avg_claim_value}\n",
    "    Personnel Cost: {personnel_cost}\n",
    "    Infra Cost: {infra_cost}\n",
    "    Compliance Cost: {compliance_cost}\n",
    "    Top Feature Contributions: {json.dumps(weighted_contributions, indent=2)}\n",
    "    \"\"\"\n",
    "\n",
    "    # <<< END ADDED >>>\n",
    "\n",
    "    explanation_response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": explanation_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    explanation = explanation_response.choices[0].message.content.strip()\n",
    "    table_response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": table_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    table_json = table_response.choices[0].message.content.strip()\n",
    "\n",
    "    cleaned_json = re.sub(r\"```json|```\", \"\", table_json).strip()\n",
    "    json_match = re.search(r\"(\\[.*\\]|\\{.*\\})\", cleaned_json, re.DOTALL)\n",
    "    if json_match:\n",
    "        cleaned_json = json_match.group(1)\n",
    "\n",
    "    try:\n",
    "        metrics_data = json.loads(cleaned_json)\n",
    "        if isinstance(metrics_data, dict):\n",
    "            metrics_data = [metrics_data]\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"Failed to parse JSON from LLM. Cleaned text:\\n{cleaned_json}\")\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    csv_file = r\"D:\\CTS Claims project\\metrics_table.csv\"\n",
    "    json_file = r\"D:\\CTS Claims project\\metrics_table.json\"\n",
    "\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    df.to_json(json_file, orient=\"records\", indent=4)\n",
    "\n",
    "    print(f\"✅ Metrics saved as:\\n- {csv_file}\\n- {json_file}\")\n",
    "\n",
    "    return explanation, metrics_data\n",
    "\n",
    "explanation, metrics_data = explain_claim_with_llm()\n",
    "\n",
    "print(\"\\n=== HUMAN-FRIENDLY EXPLANATION ===\")\n",
    "print(explanation)\n",
    "print(\"\\n=== METRICS JSON (from LLM) ===\")\n",
    "print(json.dumps(metrics_data, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12ac63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
